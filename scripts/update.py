# scripts/update.py
import json
import os
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup

PARSER_VERSION = 2
URLS = [
    {
        "platform": "BookWalker",
        "url": "https://www.bookwalker.com.tw/event",
        "note": "ä¸»é¡Œ&æ´»å‹•åˆ—è¡¨",
        "extra": "bw",
    },
    {
        "platform": "Readmoo",
        "url": "https://readmoo.com/campaign/activities",
        "note": "é€²è¡Œä¸­æ´»å‹•",
        "extra": "readmoo",
    },
    {
        "platform": "HyRead",
        "url": "https://ebook.hyread.com.tw/Template/store/event_list.jsp",
        "note": "ç†±é–€æ´»å‹•",
        "extra": "hyread",
    },
    {
        "platform": "Pubu",
        "url": "https://www.pubu.com.tw/activity/ongoing",
        "note": "å…¨ç«™æ´»å‹•",
        "extra": "pubu",
    },
    {
        "platform": "Kobo",
        "url": "https://www.kobo.com/tw/zh",
        "note": "ä¸»é ï¼ˆå¼±ä¾†æºï¼‰",
        "extra": None,
    },
    {
        "platform": "åšå®¢ä¾†",
        "url": "https://activity.books.com.tw/crosscat/show/A00000062854?loc=mood_001",
        "note": "é›»å­æ›¸æ´»å‹•å…¥å£ï¼ˆå¯èƒ½æœƒèª¿æ•´ï¼‰",
        "extra": "books"
    },
]

OUT_JSON = "data/deals.json"
OUT_HTML = "index.html"


def extract_title(html: str) -> str:
    m = re.search(r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    if not m:
        return ""
    return re.sub(r"\s+", " ", m.group(1)).strip()


def fetch_html(url: str) -> Dict[str, Any]:
    headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    }
    r = requests.get(url, headers=headers, timeout=25)
    status = r.status_code
    r.raise_for_status()
    r.encoding = r.apparent_encoding or "utf-8"
    return {"text": r.text, "status": status}


def pick_unique_texts(texts: List[str], limit: int = 8) -> List[str]:
    # 1) åŸºç¤æ¸…ç†
    cleaned = []
    for t in texts:
        t = re.sub(r"\s+", " ", (t or "")).strip()
        if not t or len(t) < 4:
            continue
        cleaned.append(t)

    # 2) å…ˆç”¨é•·åº¦æ’åºï¼šé•·çš„åœ¨å‰ï¼ˆè³‡è¨Šé‡é€šå¸¸æ¯”è¼ƒé«˜ï¼‰
    cleaned = sorted(set(cleaned), key=len, reverse=True)

    # 3) å­å­—ä¸²å»é‡ï¼šå¦‚æœ t å®Œå…¨åŒ…å«åœ¨å·²ä¿ç•™çš„æŸä¸€æ¢è£¡ï¼Œå°±ä¸Ÿæ‰
    kept = []
    for t in cleaned:
        if any(t in k for k in kept):
            continue
        kept.append(t)

    return kept[:limit]


def extract_bw_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")

    prev_titles = set()
    try:
        with open("data/deals.json", "r", encoding="utf-8") as f:
            prev = json.load(f)
        for it in prev.get("items", []):
            if it.get("platform") == "BookWalker":
                prev_titles = set(it.get("card_titles", []))
    except Exception:
        pass

    candidates = []
    for a in soup.select("a"):
        t = a.get_text(" ", strip=True)
        t = re.sub(r"\s+", " ", (t or "")).strip()
        if not t:
            continue
        if len(t) < 6 or len(t) > 90:
            continue

        # å°è¦½/ç³»çµ±å­—è¸¢æ‰
        if any(bad in t for bad in [
            "æœƒå“¡è³‡æ–™", "æœƒå“¡é€šçŸ¥", "ç™»å…¥", "è¨»å†Š", "æ¨è–¦ä¸»é¡Œ", "æ´»å‹•åˆ—è¡¨", "æŸ¥çœ‹æ›´å¤š", "ä¸‹è¼‰APP"
        ]):
            continue

        candidates.append(t)

    def score(t: str) -> int:
        s = 0
        # æŠ˜æ‰£/åƒ¹æ ¼/é–€æª»/æ—¥æœŸï¼šå¼·è¨Šè™Ÿ
        if re.search(r"\d+\s*æŠ˜", t): s += 6
        if re.search(r"\d+\s*(%|ï¼…)", t): s += 5
        if re.search(r"æ»¿\s*\d+", t): s += 5
        if re.search(r"ç‰¹åƒ¹\s*\d+|å„ªæƒ åƒ¹\s*\d+|\d+\s*å…ƒ", t): s += 5  # 99å…ƒã€2000å…ƒ
        if re.search(r"\d{1,2}[./]\d{1,2}", t): s += 4
        if re.search(r"\d{4}[./]\d{1,2}[./]\d{1,2}", t): s += 4
        if any(k in t for k in ["é™æ™‚", "å„ªæƒ ", "æŠ˜åƒ¹åˆ¸", "å›é¥‹", "æ›¸å±•", "å†æŠ˜", "åŠ ç¢¼", "ç‰¹åƒ¹"]): s += 3

        # æ´»å‹•å‹ï¼ˆä½ å‰›å‰›é»åçš„é–±è®€å ±å‘Š/é»æ•¸åˆ¸ï¼‰ï¼šåŠ åˆ†è®“å®ƒä¸æœƒè¢«æ“ æ‰
        if any(k in t for k in ["é–±è®€å ±å‘Š", "é»æ•¸", "é ˜åˆ¸", "å„ªæƒ åˆ¸", "æŠ½ç", "ä»»å‹™"]): s += 6

        # ä¸è¦å†ç”¨ã€Œæ—¥æ–‡æ›¸ã€æ‰£åˆ†ï¼ˆå®ƒæœ‰æ™‚å°±æ˜¯æ­£å¸¸æ´»å‹•æ¨™é¡Œçš„ä¸€éƒ¨åˆ†ï¼‰
        # ä½†ä»ç„¶ä¿ç•™å°ã€Œé™åˆ¶ç´š/é€£è¼‰ã€çš„è² åˆ†ä»¥é¿å…æ´—ç‰ˆ
        if any(k in t for k in ["é™åˆ¶ç´š", "é€£è¼‰"]): s -= 6

        return s

    scored = [(score(t), t) for t in candidates]
    scored = [(sc, t) for (sc, t) in scored if sc > 0]
    scored.sort(key=lambda x: x[0], reverse=True)
    # æ–°æ´»å‹•ï¼šä»Šå¤©æœ‰ã€æ˜¨å¤©æ²’æœ‰
    new_items = [t for (_, t) in scored if t not in prev_titles]

    # ä¿è­‰æœ€å¤šä¿ç•™ 2 å€‹æ–°æ´»å‹•
    guaranteed_new = new_items[:2]

    # åˆ†å…©é¡ï¼šä¿ƒéŠ·å‹ vs æ´»å‹•å‹
    promo_like = []
    activity_like = []
    for sc, t in scored:
        if any(k in t for k in ["é–±è®€å ±å‘Š", "é»æ•¸", "é ˜åˆ¸", "å„ªæƒ åˆ¸", "æŠ½ç", "ä»»å‹™"]):
            activity_like.append(t)
        else:
            promo_like.append(t)

    # ä¿ƒéŠ·å‹å…ˆå– 6ï¼Œæ´»å‹•å‹è£œ 2ï¼ˆé¿å…æ¼æ‰ä½ åœ¨æ„çš„æ´»å‹•ï¼‰
    picked = guaranteed_new + promo_like

    return pick_unique_texts(picked, limit=15)


def extract_readmoo_cards(html: str) -> List[str]:
    import re
    import json

    # Readmoo å¸¸è¦‹å…©ç¨®ç‹€æ…‹ï¼š
    # A) æœ‰ READMOO_CAMPAIGNSï¼ˆå¯æŠ½ï¼‰
    # B) è¢«æ“‹ï¼ˆåªæœ‰é©—è­‰/JS æç¤ºé ï¼‰ -> æŠ½ä¸åˆ°
    h_lower = (html or "").lower()
    if "verify that you're not a robot" in h_lower or "enable javascript" in h_lower:
        return []

    # æ”¾å¯¬ï¼šæŠ“åˆ°ç¬¬ä¸€å€‹ ]; ç‚ºæ­¢ï¼Œä¸è¦æ±‚ä¸€å®šæ˜¯ [{...}];
    m = re.search(r"const\s+READMOO_CAMPAIGNS\s*=\s*(\[[\s\S]*?\]);", html)
    if not m:
        return []

    raw = m.group(1)
    try:
        data = json.loads(raw)
    except Exception:
        return []

    cards = []
    for item in data:
        name = (item.get("name") or "").strip()
        desc = (item.get("description") or "").strip()
        start = (item.get("start_date") or "").strip()
        end = (item.get("end_date") or "").strip()

        line = " ".join(x for x in [
            name,
            desc,
            f"{start}â€“{end}" if start or end else ""
        ] if x)

        if line:
            cards.append(line)

    return cards

def extract_hyread_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")

    results = []
    seen = set()

    # HyRead æ´»å‹•é é€šå¸¸æ˜¯ä¸€å¼µå¼µå¡ç‰‡ï¼Œæ¯å¼µå¡ç‰‡å…§æœ‰ã€Œä¸»æ¨™é¡Œã€+ã€Œç´…å­—å‰¯æ¨™(æ—¥æœŸ/æŠ˜æ‰£)ã€
    # æˆ‘å€‘åšæ³•ï¼šå¾æ¯å€‹ <a> å¾€ä¸Šæ‰¾å¡ç‰‡å®¹å™¨ï¼ŒæŠ½ã€Œæœ€åƒä¸»æ¨™ã€èˆ‡ã€Œæœ€åƒæŠ˜æ‰£/æ—¥æœŸã€çš„å…©æ®µã€‚
    for a in soup.select("a[href]"):
        href = (a.get("href") or "").strip()
        if not href:
            continue

        # é€™æ¢è¦å‰‡ç”¨ä¾†é¿å…æŠ“åˆ°å°è¦½é€£çµï¼›è‹¥ä½ ç™¼ç¾æŠ“å¤ªå°‘ï¼Œå¯æŠŠé€™è¡Œè¨»è§£æ‰
        if "event" not in href.lower():
            continue

        # å¾€ä¸Šæ‰¾å®¹å™¨ï¼ˆæœ€å¤šçˆ¬ 5 å±¤ï¼‰ï¼ŒæŠ“ä¸€æ•´å¼µå¡ç‰‡çš„æ–‡å­—
        node = a
        container_texts = None
        for _ in range(5):
            if node and node.name in ("div", "li", "article", "section"):
                texts = [t.strip() for t in node.stripped_strings if t.strip()]
                # å¡ç‰‡é€šå¸¸è‡³å°‘æœƒæœ‰ 2 æ®µæ–‡å­—
                if len(texts) >= 2:
                    container_texts = texts
                    break
            node = node.parent

        if not container_texts:
            continue

        # ä¸»æ¨™ï¼šæŒ‘ã€Œæ¯”è¼ƒä¸åƒæ—¥æœŸ/æŠ˜æ‰£ã€ä¸”å­—æ•¸åˆç†çš„å¥å­
        # å‰¯æ¨™ï¼šæŒ‘ã€Œå«æŠ˜/å…ƒ/%/æ»¿/æ—¥æœŸç¬¦è™Ÿã€çš„å¥å­
        def looks_like_meta(s: str) -> bool:
            return bool(re.search(r"(æŠ˜|å…ƒ|%|ï¼…|æ»¿|å†æŠ˜|é™æ™‚|é€±æœ«|é™å®š|\d{1,2}[./-]\d{1,2}|/)", s))

        title = ""
        subtitle = ""

        # å…ˆæ‰¾å‰¯æ¨™ï¼ˆé€šå¸¸ç´…å­—é‚£è¡Œï¼‰
        meta_candidates = [t for t in container_texts if looks_like_meta(t) and len(t) <= 60]
        if meta_candidates:
            # é€šå¸¸ç¬¬ä¸€å€‹å°±å¾ˆåƒæ—¥æœŸ/æŠ˜æ‰£
            subtitle = meta_candidates[0]

        # å†æ‰¾ä¸»æ¨™
        title_candidates = [t for t in container_texts if (not looks_like_meta(t)) and 4 <= len(t) <= 30]
        if title_candidates:
            title = title_candidates[0]
        else:
            # æ‰¾ä¸åˆ°å°±é€€è€Œæ±‚å…¶æ¬¡ï¼šå–ç¬¬ä¸€å€‹çŸ­å¥ç•¶ä¸»æ¨™
            short = [t for t in container_texts if 4 <= len(t) <= 30]
            title = short[0] if short else ""

        title = re.sub(r"\s+", " ", title).strip()
        subtitle = re.sub(r"\s+", " ", subtitle).strip()

        if not title:
            continue

        line = f"{title}ï½œ{subtitle}" if subtitle else title
        # é¿å…è¶…é•·
        if len(line) > 90:
            line = line[:87] + "â€¦"

        if line not in seen:
            seen.add(line)
            results.append(line)

    return pick_unique_texts(results, limit=12)

def extract_books_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    for a in soup.select("a"):
        txt = a.get_text(" ", strip=True)
        txt = re.sub(r"\s+", " ", (txt or "")).strip()
        if not txt:
            continue
        if len(txt) < 6 or len(txt) > 80:
            continue

        if any(bad in txt for bad in ["ç™»å…¥", "è¨»å†Š", "æœƒå“¡", "è³¼ç‰©è»Š", "å®¢æœ", "æ›´å¤š", "è¿”å›"]):
            continue

        # åªä¿ç•™æ¯”è¼ƒåƒæ´»å‹•çš„
        if any(k in txt for k in ["æŠ˜", "å„ªæƒ ", "æ´»å‹•", "æ›¸å±•", "ç‰¹åƒ¹", "å›é¥‹", "æ»¿", "é™æ™‚"]):
            candidates.append(txt)

    return pick_unique_texts(candidates, limit=12)

def extract_pubu_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    results = []
    seen = set()

    # Pubu å¡ç‰‡å¸¸è¦‹æœ‰ã€Œæ´»å‹•æœŸé–“ 2025-xx-xx - 2026-xx-xxã€
    date_re = re.compile(r"\d{4}-\d{2}-\d{2}")

    # åšæ³•ï¼šæ‰¾å‡ºåŒ…å«æ—¥æœŸçš„æ–‡å­—å€å¡Šï¼Œå¾€ä¸Šæ‰¾å¡ç‰‡å®¹å™¨ï¼Œå†æŠ½å‡ºæ¨™é¡Œ
    for el in soup.find_all(string=lambda s: s and date_re.search(s)):
        txt = re.sub(r"\s+", " ", str(el)).strip()
        # åªè™•ç†çœ‹èµ·ä¾†åƒã€Œæ´»å‹•æœŸé–“ã€é‚£ç¨®
        if "æ´»å‹•æœŸé–“" not in txt and "æ´»å‹•æ™‚é–“" not in txt and len(date_re.findall(txt)) < 2:
            continue

        # å¾€ä¸Šæ‰¾å¡ç‰‡å®¹å™¨
        node = el.parent
        container = None
        for _ in range(6):
            if not node:
                break
            if node.name in ("div", "li", "article", "section"):
                texts = [t.strip() for t in node.stripped_strings if t.strip()]
                # å¡ç‰‡æ‡‰è©²æœƒæœ‰æ¨™é¡Œ + æœŸé–“ï¼Œè‡³å°‘ 2 æ®µ
                if len(texts) >= 2:
                    container = node
                    break
            node = node.parent

        if not container:
            continue

        texts = [t.strip() for t in container.stripped_strings if t.strip()]

        # å˜—è©¦æ‰¾æ¨™é¡Œï¼šé€šå¸¸æ˜¯ç¬¬ä¸€å€‹æ¯”è¼ƒçŸ­ã€ä¸”ä¸åŒ…å«æ—¥æœŸ/æ´»å‹•æœŸé–“çš„å¥å­
        title = ""
        for t in texts:
            if len(t) <= 40 and (not date_re.search(t)) and ("æ´»å‹•æœŸé–“" not in t) and ("æ´»å‹•æ™‚é–“" not in t):
                title = t
                break

        # æŠ“æœŸé–“ï¼šæ‰¾ç¬¬ä¸€å€‹å«å…©å€‹æ—¥æœŸçš„å¥å­
        period = ""
        for t in texts:
            dates = date_re.findall(t)
            if len(dates) >= 2:
                # å¯èƒ½æ˜¯ "æ´»å‹•æœŸé–“2025-..-.. - 2026-..-.."
                period = re.sub(r"\s+", " ", t).strip()
                break

        if not title:
            continue

        line = f"{title}ï½œ{period}" if period else title
        if len(line) > 100:
            line = line[:97] + "â€¦"

        if line not in seen:
            seen.add(line)
            results.append(line)

    return pick_unique_texts(results, limit=12)

def load_prev_signature() -> Dict[str, Any]:
    if not os.path.exists(OUT_JSON):
        return {"parser_version": None, "sig": {}}
    try:
        with open(OUT_JSON, "r", encoding="utf-8") as f:
            prev = json.load(f)
        sig = {}
        for it in prev.get("items", []):
            platform = it.get("platform", "")
            signature = it.get("signature", "")
            if platform:
                sig[platform] = signature
        return {"parser_version": prev.get("parser_version"), "sig": sig}
    except Exception:
        return {"parser_version": None, "sig": {}}


def make_signature(page_title: str, card_titles: List[str], status: int, error: str) -> str:
    # ç”¨ã€Œæœ€èƒ½ä»£è¡¨ä»Šæ—¥ç‹€æ…‹ã€çš„è³‡è¨Šåšç°½å
    base = {
        "status": status,
        "title": page_title or "",
        "cards": card_titles[:8],
        "error": (error or "")[:120],
    }
    return json.dumps(base, ensure_ascii=False, sort_keys=True)


def main():
    tz = timezone(timedelta(hours=8))  # å°ç£æ™‚é–“
    now = datetime.now(tz).strftime("%Y-%m-%d %H:%M")

    prev = load_prev_signature()
    prev_sig = prev["sig"]
    prev_ver = prev["parser_version"]

    items = []
    changed_platforms = []

    for x in URLS:
        title = ""
        error = ""
        status = 0
        card_titles: List[str] = []

        try:
            res = fetch_html(x["url"])
            html = res["text"]
            status = res["status"]
            title = extract_title(html)

            # ğŸ” DEBUGï¼šHTML å¤ªçŸ­æ™‚å­˜æª”ï¼ˆåˆ¤æ–·æ˜¯å¦è¢«æ“‹ï¼‰
            if html and len(html) < 2000:
                from pathlib import Path
                slug = x["platform"].lower()
                Path(f"debug_{slug}.html").write_text(html, encoding="utf-8")

            if x.get("extra") == "bw":
                card_titles = extract_bw_cards(html)
            elif x.get("extra") == "readmoo":
                card_titles = extract_readmoo_cards(html)
            elif x.get("extra") == "hyread":
                card_titles = extract_hyread_cards(html)
            elif x.get("extra") == "books":
                card_titles = extract_books_cards(html)
            elif x.get("extra") == "pubu":
                card_titles = extract_pubu_cards(html)

        except requests.HTTPError as e:
            # ä¾‹å¦‚ 403
            error = str(e)
            try:
                status = e.response.status_code if e.response is not None else 0
            except Exception:
                status = 0
        except Exception as e:
            error = str(e)

        signature = make_signature(title, card_titles, status, error)

        if (
            prev_ver == PARSER_VERSION
            and prev_sig.get(x["platform"])
            and prev_sig.get(x["platform"]) != signature
        ):
            changed_platforms.append(x["platform"])

        if x["platform"] == "Readmoo":
            # å¦‚æœæŠ½ä¸åˆ°æ´»å‹•ï¼Œé †ä¾¿åˆ¤æ–·æ˜¯ä¸æ˜¯è¢«æ“‹
            if ("READMOO_CAMPAIGNS" not in html) and (
                "verify that you're not a robot" in html.lower()
                or "enable javascript" in html.lower()
            ):
                error = "Readmoo ç–‘ä¼¼åæ©Ÿå™¨äºº/JS é©—è­‰ï¼ŒActions æŠ“åˆ°çš„ä¸æ˜¯æ´»å‹•é å…§å®¹"

        blocked = False
        blocked_reason = ""

        if x["platform"] == "Readmoo":
            # åªè¦æŠ“åˆ°çš„ä¸æ˜¯æ´»å‹•é æœ¬é«”ï¼Œå°±è¦–ç‚º blocked
            if error and ("robot" in error.lower() or "javascript" in error.lower() or "js" in error.lower()):
                blocked = True
                blocked_reason = "éœ€è¦ JavaScript é©—è­‰ï¼ŒActions ç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"
        else:
            # æ²’æœ‰ error ä¹Ÿå¯èƒ½æ‹¿åˆ°é©—è­‰é ï¼ˆ200 OKï¼‰
            h = (html or "").lower() if "html" in locals() else ""
            if "verify that you're not a robot" in h or "enable javascript" in h:
                blocked = True
                blocked_reason = "éœ€è¦ JavaScript é©—è­‰ï¼ŒActions ç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"
            # æˆ–æ˜¯æ ¹æœ¬æ²’æœ‰ READMOO_CAMPAIGNSï¼ˆä½ èµ° JS è®Šæ•¸æŠ½å–é‚£æ¢è·¯æ™‚å¾ˆæœ‰ç”¨ï¼‰
            if (not blocked) and ("readmoo_campaigns" not in h):
            # é€™æ¢æ¯”è¼ƒä¿å®ˆï¼šåªæœ‰ç•¶ card_titles ä¹Ÿç©ºæ‰åˆ¤å®š
                if not card_titles:
                    blocked = True
                    blocked_reason = "ç–‘ä¼¼åæ©Ÿå™¨äºº/JS é©—è­‰ï¼Œç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"

        if x.get("extra") is None and x["platform"] not in ("Kobo", "Pubu"):  
            cards = ["ï¼ˆæœªè¨­å®šè§£æå™¨ extraï¼Œæš«æ™‚ä¸æœƒè§£æå‡ºæ´»å‹•ï¼‰"]

        # åšå®¢ä¾†ï¼šå…¥å£æ¨¡å¼ï¼ˆä¸é¡¯ç¤ºæ“·å–å¡ç‰‡ï¼Œé¿å…è¢«å•†å“/å¥—çµ„æ´—ç‰ˆï¼‰
        if x["platform"] == "åšå®¢ä¾†":
            blocked = True
            blocked_reason = "å…¥å£æ¨¡å¼ï¼šåšå®¢ä¾†æ´»å‹•é è³‡è¨Šæµé›œè¨Šé«˜ï¼Œv1 å…ˆåªä¿ç•™å…¥å£é€£çµ"

        items.append(
            {
                "platform": x["platform"],
                "url": x["url"],
                "note": x["note"],
                "page_title": title,
                "card_titles": card_titles,
                "http_status": status,
                "error": error,
                "signature": signature,
                "blocked": blocked,
                "blocked_reason": blocked_reason,
            }
        )

    os.makedirs("data", exist_ok=True)

    payload = {
        "parser_version": PARSER_VERSION,
        "updated_at_taipei": now,
        "has_new_changes": "æ˜¯" if len(changed_platforms) > 0 else "å¦",
        "changed_platforms": changed_platforms,
        "items": items,
    }

    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    # HTML ä¸€é æ¸…å–®ï¼ˆå«å¼±åŒ– 403ï¼‰
    html_lines = []
    html_lines.append("<!doctype html>")
    html_lines.append('<html lang="zh-Hant">')
    html_lines.append("<head>")
    html_lines.append('<meta charset="utf-8" />')
    html_lines.append('<meta name="viewport" content="width=device-width, initial-scale=1" />')
    html_lines.append("<title>é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</title>")
    html_lines.append("</head>")
    html_lines.append("<body style='font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Noto Sans TC,Helvetica,Arial;line-height:1.65;padding:16px;max-width:980px;margin:0 auto;'>")
    html_lines.append("<h1 style='margin:0 0 8px;'>ğŸ“š é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</h1>")
    html_lines.append(f"<p style='margin:0 0 8px;'>æ›´æ–°æ™‚é–“ï¼ˆå°ç£ï¼‰ï¼š<b>{payload['updated_at_taipei']}</b></p>")
    html_lines.append(f"<p style='margin:0 0 16px;'>ä»Šå¤©æ˜¯å¦æœ‰æ–°å¢æ´»å‹•ï¼š<b>{payload['has_new_changes']}</b>"
                      + (f"ï¼ˆè®Šå‹•ï¼š{', '.join(payload['changed_platforms'])}ï¼‰" if payload["changed_platforms"] else "")
                      + "</p>")
    html_lines.append("<hr style='opacity:.35'/>")

    for it in items:
        is_403 = (it.get("http_status") == 403) or ("403" in (it.get("error") or ""))
        # å¼±åŒ–ï¼š403 è®Šç° + é™ä½é€æ˜åº¦
        wrap_style = "opacity:.45; filter: grayscale(1);" if is_403 else "opacity:1;"
        title_style = "color:#555;" if is_403 else "color:#111;"

        html_lines.append(f"<section style='{wrap_style} padding:8px 0;'>")
        html_lines.append(f"<h2 style='margin:6px 0; {title_style}'>{it['platform']}</h2>")

        if it["page_title"]:
            html_lines.append(f"<p style='margin:4px 0;'>é é¢æ¨™é¡Œï¼š{it['page_title']}</p>")
        if it["note"]:
            html_lines.append(f"<p style='margin:4px 0;'>å‚™è¨»ï¼š{it['note']}</p>")

        is_blocked = bool(it.get("blocked"))

        # æ¨¡å¼ 3ï¼šReadmoo / åšå®¢ä¾† è‹¥ blockedï¼Œå°±ä¸é¡¯ç¤ºå¡ç‰‡å€å¡Šï¼Œåªé¡¯ç¤ºåŸå› ï¼‹é€£çµ
        if it["platform"] in ("Readmoo", "åšå®¢ä¾†") and is_blocked:
            reason = it.get("blocked_reason") or "å…¥å£æ¨¡å¼"
            html_lines.append(f"<p style='margin:6px 0; color:#666;'>ï¼ˆ{reason}ï¼‰</p>")
        else:
            if it.get("card_titles"):
                html_lines.append("<div style='margin:8px 0 6px;'><b>æ´»å‹•å¡ç‰‡ï¼ˆæ“·å–ï¼‰</b></div>")
                html_lines.append("<ul style='margin:6px 0 10px 18px;'>")
                for t in it["card_titles"][:10]:
                    html_lines.append(f"<li>{t}</li>")
                html_lines.append("</ul>")

        # errorï¼šè‹¥æ˜¯ Readmoo/åšå®¢ä¾†å…¥å£æ¨¡å¼ï¼Œå°±ä¸è¦ç”¨ç´…å­—åš‡äººï¼ˆåŸå› å·²ç¶“é¡¯ç¤ºï¼‰
        if it.get("error") and not (it["platform"] in ("Readmoo", "åšå®¢ä¾†") and is_blocked):
            html_lines.append(
                f"<p style='margin:6px 0; color:#b00020;'>ï¼ˆæŠ“å–å¤±æ•—ï¼š{it['error']}ï¼‰</p>"
            )
       

        html_lines.append(f"<p style='margin:6px 0;'><a href='{it['url']}' target='_blank' rel='noopener noreferrer'>â†’ é»æˆ‘æŸ¥çœ‹æ´»å‹•</a></p>")



        html_lines.append("</section>")
        html_lines.append("<hr style='opacity:.25'/>")

    html_lines.append("<p style='font-size:12px;opacity:.7;margin-top:14px;'>v1 åªå½™æ•´å®˜æ–¹æ´»å‹•å…¥å£ï¼›ä¸è¨ˆåˆ¸å¾Œåƒ¹èˆ‡å–®æ›¸ç‰¹åƒ¹ã€‚403 å¹³å°å·²è‡ªå‹•å¼±åŒ–é¡¯ç¤ºã€‚</p>")
    html_lines.append("</body></html>")

    with open(OUT_HTML, "w", encoding="utf-8") as f:
        f.write("\n".join(html_lines))


if __name__ == "__main__":
    main()
