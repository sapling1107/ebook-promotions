# scripts/update.py
import json
import os
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup

URLS = [
    {
        "platform": "BookWalker",
        "url": "https://www.bookwalker.com.tw/event",
        "note": "ä¸»é¡Œ&æ´»å‹•åˆ—è¡¨",
        "extra": "bw",
    },
    {
        "platform": "Readmoo",
        "url": "https://readmoo.com/campaign/activities",
        "note": "é€²è¡Œä¸­æ´»å‹•",
        "extra": "readmoo",
    },
    {
        "platform": "HyRead",
        "url": "https://ebook.hyread.com.tw/Template/store/event_list.jsp",
        "note": "ç†±é–€æ´»å‹•",
        "extra": None,
    },
    {
        "platform": "Pubu",
        "url": "https://www.pubu.com.tw/activity/ongoing",
        "note": "å…¨ç«™æ´»å‹•",
        "extra": None,
    },
    {
        "platform": "Kobo",
        "url": "https://www.kobo.com/tw/zh",
        "note": "ä¸»é ï¼ˆå¼±ä¾†æºï¼‰",
        "extra": None,
    },
    {
        "platform": "åšå®¢ä¾†",
        "url": "https://activity.books.com.tw/crosscat/show/A00000062854?loc=mood_001",
        "note": "é›»å­æ›¸æ´»å‹•å…¥å£ï¼ˆå¯èƒ½æœƒèª¿æ•´ï¼‰",
        "extra": None,
    },
]

OUT_JSON = "data/deals.json"
OUT_HTML = "index.html"


def extract_title(html: str) -> str:
    m = re.search(r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    if not m:
        return ""
    return re.sub(r"\s+", " ", m.group(1)).strip()


def fetch_html(url: str) -> Dict[str, Any]:
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; ebook-promotions-bot/1.0)"
    }
    r = requests.get(url, headers=headers, timeout=25)
    status = r.status_code
    r.raise_for_status()
    r.encoding = r.apparent_encoding or "utf-8"
    return {"text": r.text, "status": status}


def pick_unique_texts(texts: List[str], limit: int = 8) -> List[str]:
    out = []
    seen = set()
    for t in texts:
        t = re.sub(r"\s+", " ", (t or "")).strip()
        if not t:
            continue
        if len(t) < 3:
            continue
        if t in seen:
            continue
        seen.add(t)
        out.append(t)
        if len(out) >= limit:
            break
    return out


def extract_bw_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")

    # å…ˆæŠ“ã€Œçœ‹èµ·ä¾†åƒæ´»å‹•å¡ç‰‡/åˆ—è¡¨ã€çš„é€£çµæ–‡å­—
    candidates = []

    # å¸¸è¦‹ï¼šæ´»å‹•åˆ—è¡¨æœƒæœ‰å¾ˆå¤š <a> çš„å¯è¦‹æ–‡å­—
    for a in soup.select("a"):
        txt = a.get_text(" ", strip=True)
        # éæ¿¾æ‰å°è¦½ã€ç™»å…¥ã€å¸¸è¦‹ç„¡æ„ç¾©é€£çµ
        if not txt:
            continue
        if txt in {"é»æˆ‘æŸ¥çœ‹æ´»å‹•", "æ›´å¤š", "è¿”å›", "ç™»å…¥", "è¨»å†Š"}:
            continue
        # éæ¿¾éé•·çš„æ®µè½å‹æ–‡å­—
        if len(txt) > 60:
            continue
        candidates.append(txt)

    # å†ç”¨ä¸€äº›å¸¸è¦‹é—œéµå­—æå‡å‘½ä¸­ç‡ï¼ˆä¸ç¡¬æ€§ä¾è³´ï¼‰
    boosted = [t for t in candidates if any(k in t for k in ["æŠ˜", "æ»¿", "æœƒå“¡", "å„ªæƒ ", "æ´»å‹•", "æ›¸å±•", "é™å®š", "å›é¥‹"])]
    merged = boosted + candidates

    return pick_unique_texts(merged, limit=10)


def extract_readmoo_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    # Readmoo æ´»å‹•é é€šå¸¸æœ‰æ´»å‹•å¡ç‰‡æ¨™é¡Œï¼Œç›´æ¥æŠ“æ‰€æœ‰ <a>/<h*> çš„çŸ­æ–‡å­—
    for tag in soup.select("h1, h2, h3, h4, a"):
        txt = tag.get_text(" ", strip=True)
        if not txt:
            continue
        if txt in {"é»æˆ‘æŸ¥çœ‹æ´»å‹•", "æ›´å¤š", "è¿”å›", "ç™»å…¥", "è¨»å†Š"}:
            continue
        if len(txt) > 60:
            continue
        candidates.append(txt)

    boosted = [t for t in candidates if any(k in t for k in ["æŠ˜", "æ»¿", "æœƒå“¡", "å„ªæƒ ", "æ´»å‹•", "æ›¸å±•", "å›é¥‹", "é™æ™‚"])]
    merged = boosted + candidates
    return pick_unique_texts(merged, limit=10)


def load_prev_signature() -> Dict[str, str]:
    if not os.path.exists(OUT_JSON):
        return {}
    try:
        with open(OUT_JSON, "r", encoding="utf-8") as f:
            prev = json.load(f)
        sig = {}
        for it in prev.get("items", []):
            platform = it.get("platform", "")
            signature = it.get("signature", "")
            if platform:
                sig[platform] = signature
        return sig
    except Exception:
        return {}


def make_signature(page_title: str, card_titles: List[str], status: int, error: str) -> str:
    # ç”¨ã€Œæœ€èƒ½ä»£è¡¨ä»Šæ—¥ç‹€æ…‹ã€çš„è³‡è¨Šåšç°½å
    base = {
        "status": status,
        "title": page_title or "",
        "cards": card_titles[:8],
        "error": (error or "")[:120],
    }
    return json.dumps(base, ensure_ascii=False, sort_keys=True)


def main():
    tz = timezone(timedelta(hours=8))  # å°ç£æ™‚é–“
    now = datetime.now(tz).strftime("%Y-%m-%d %H:%M")

    prev_sig = load_prev_signature()

    items = []
    changed_platforms = []

    for x in URLS:
        title = ""
        error = ""
        status = 0
        card_titles: List[str] = []

        try:
            res = fetch_html(x["url"])
            html = res["text"]
            status = res["status"]
            title = extract_title(html)

            if x.get("extra") == "bw":
                card_titles = extract_bw_cards(html)
            elif x.get("extra") == "readmoo":
                card_titles = extract_readmoo_cards(html)

        except requests.HTTPError as e:
            # ä¾‹å¦‚ 403
            error = str(e)
            try:
                status = e.response.status_code if e.response is not None else 0
            except Exception:
                status = 0
        except Exception as e:
            error = str(e)

        signature = make_signature(title, card_titles, status, error)

        if prev_sig.get(x["platform"]) and prev_sig.get(x["platform"]) != signature:
            changed_platforms.append(x["platform"])

        items.append(
            {
                "platform": x["platform"],
                "url": x["url"],
                "note": x["note"],
                "page_title": title,
                "card_titles": card_titles,
                "http_status": status,
                "error": error,
                "signature": signature,
            }
        )

    os.makedirs("data", exist_ok=True)

    payload = {
        "updated_at_taipei": now,
        "has_new_changes": "æ˜¯" if len(changed_platforms) > 0 else "å¦",
        "changed_platforms": changed_platforms,
        "items": items,
    }

    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    # HTML ä¸€é æ¸…å–®ï¼ˆå«å¼±åŒ– 403ï¼‰
    html_lines = []
    html_lines.append("<!doctype html>")
    html_lines.append('<html lang="zh-Hant">')
    html_lines.append("<head>")
    html_lines.append('<meta charset="utf-8" />')
    html_lines.append('<meta name="viewport" content="width=device-width, initial-scale=1" />')
    html_lines.append("<title>é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</title>")
    html_lines.append("</head>")
    html_lines.append("<body style='font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Noto Sans TC,Helvetica,Arial;line-height:1.65;padding:16px;max-width:980px;margin:0 auto;'>")
    html_lines.append("<h1 style='margin:0 0 8px;'>ğŸ“š é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</h1>")
    html_lines.append(f"<p style='margin:0 0 8px;'>æ›´æ–°æ™‚é–“ï¼ˆå°ç£ï¼‰ï¼š<b>{payload['updated_at_taipei']}</b></p>")
    html_lines.append(f"<p style='margin:0 0 16px;'>ä»Šå¤©æ˜¯å¦æœ‰æ–°å¢æ´»å‹•ï¼š<b>{payload['has_new_changes']}</b>"
                      + (f"ï¼ˆè®Šå‹•ï¼š{', '.join(payload['changed_platforms'])}ï¼‰" if payload["changed_platforms"] else "")
                      + "</p>")
    html_lines.append("<hr style='opacity:.35'/>")

    for it in items:
        is_403 = (it.get("http_status") == 403) or ("403" in (it.get("error") or ""))
        # å¼±åŒ–ï¼š403 è®Šç° + é™ä½é€æ˜åº¦
        wrap_style = "opacity:.45; filter: grayscale(1);" if is_403 else "opacity:1;"
        title_style = "color:#555;" if is_403 else "color:#111;"

        html_lines.append(f"<section style='{wrap_style} padding:8px 0;'>")
        html_lines.append(f"<h2 style='margin:6px 0; {title_style}'>{it['platform']}</h2>")

        if it["page_title"]:
            html_lines.append(f"<p style='margin:4px 0;'>é é¢æ¨™é¡Œï¼š{it['page_title']}</p>")
        if it["note"]:
            html_lines.append(f"<p style='margin:4px 0;'>å‚™è¨»ï¼š{it['note']}</p>")

        # é¡å¤–æŠ“å¡ç‰‡æ¨™é¡Œï¼ˆåªåœ¨ BW/Readmoo æœ‰ï¼‰
        if it.get("card_titles"):
            html_lines.append("<div style='margin:8px 0 6px;'><b>æ´»å‹•å¡ç‰‡ï¼ˆæ“·å–ï¼‰</b></div>")
            html_lines.append("<ul style='margin:6px 0 10px 18px;'>")
            for t in it["card_titles"][:10]:
                html_lines.append(f"<li>{t}</li>")
            html_lines.append("</ul>")

        html_lines.append(f"<p style='margin:6px 0;'><a href='{it['url']}' target='_blank' rel='noopener noreferrer'>â†’ é»æˆ‘æŸ¥çœ‹æ´»å‹•</a></p>")

        if it["error"]:
            html_lines.append(f"<p style='margin:6px 0; color:#b00020;'>ï¼ˆæŠ“å–å¤±æ•—ï¼š{it['error']}ï¼‰</p>")

        html_lines.append("</section>")
        html_lines.append("<hr style='opacity:.25'/>")

    html_lines.append("<p style='font-size:12px;opacity:.7;margin-top:14px;'>v1 åªå½™æ•´å®˜æ–¹æ´»å‹•å…¥å£ï¼›ä¸è¨ˆåˆ¸å¾Œåƒ¹èˆ‡å–®æ›¸ç‰¹åƒ¹ã€‚403 å¹³å°å·²è‡ªå‹•å¼±åŒ–é¡¯ç¤ºã€‚</p>")
    html_lines.append("</body></html>")

    with open(OUT_HTML, "w", encoding="utf-8") as f:
        f.write("\n".join(html_lines))


if __name__ == "__main__":
    main()
