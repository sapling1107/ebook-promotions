# scripts/update.py
import json
import os
import re
from datetime import datetime, timezone, timedelta
from typing import List, Dict, Any

import requests
from bs4 import BeautifulSoup

PARSER_VERSION = 2
URLS = [
    {
        "platform": "BookWalker",
        "url": "https://www.bookwalker.com.tw/event",
        "note": "ä¸»é¡Œ&æ´»å‹•åˆ—è¡¨",
        "extra": "bw",
    },
    {
        "platform": "Readmoo",
        "url": "https://readmoo.com/campaign/activities",
        "note": "é€²è¡Œä¸­æ´»å‹•",
        "extra": "readmoo",
    },
    {
        "platform": "HyRead",
        "url": "https://ebook.hyread.com.tw/Template/store/event_list.jsp",
        "note": "ç†±é–€æ´»å‹•",
        "extra": None,
    },
    {
        "platform": "Pubu",
        "url": "https://www.pubu.com.tw/activity/ongoing",
        "note": "å…¨ç«™æ´»å‹•",
        "extra": None,
    },
    {
        "platform": "Kobo",
        "url": "https://www.kobo.com/tw/zh",
        "note": "ä¸»é ï¼ˆå¼±ä¾†æºï¼‰",
        "extra": None,
    },
    {
        "platform": "åšå®¢ä¾†",
        "url": "https://activity.books.com.tw/crosscat/show/A00000062854?loc=mood_001",
        "note": "é›»å­æ›¸æ´»å‹•å…¥å£ï¼ˆå¯èƒ½æœƒèª¿æ•´ï¼‰",
        "extra": None,
    },
]

OUT_JSON = "data/deals.json"
OUT_HTML = "index.html"


def extract_title(html: str) -> str:
    m = re.search(r"<title[^>]*>(.*?)</title>", html, re.IGNORECASE | re.DOTALL)
    if not m:
        return ""
    return re.sub(r"\s+", " ", m.group(1)).strip()


def fetch_html(url: str) -> Dict[str, Any]:
    headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "zh-TW,zh;q=0.9,en;q=0.6",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
    }
    r = requests.get(url, headers=headers, timeout=25)
    status = r.status_code
    r.raise_for_status()
    r.encoding = r.apparent_encoding or "utf-8"
    return {"text": r.text, "status": status}


def pick_unique_texts(texts: List[str], limit: int = 8) -> List[str]:
    # 1) åŸºç¤æ¸…ç†
    cleaned = []
    for t in texts:
        t = re.sub(r"\s+", " ", (t or "")).strip()
        if not t or len(t) < 4:
            continue
        cleaned.append(t)

    # 2) å…ˆç”¨é•·åº¦æ’åºï¼šé•·çš„åœ¨å‰ï¼ˆè³‡è¨Šé‡é€šå¸¸æ¯”è¼ƒé«˜ï¼‰
    cleaned = sorted(set(cleaned), key=len, reverse=True)

    # 3) å­å­—ä¸²å»é‡ï¼šå¦‚æœ t å®Œå…¨åŒ…å«åœ¨å·²ä¿ç•™çš„æŸä¸€æ¢è£¡ï¼Œå°±ä¸Ÿæ‰
    kept = []
    for t in cleaned:
        if any(t in k for k in kept):
            continue
        kept.append(t)

    return kept[:limit]


def extract_bw_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")

    prev_titles = set()
    try:
        with open("data/deals.json", "r", encoding="utf-8") as f:
            prev = json.load(f)
        for it in prev.get("items", []):
            if it.get("platform") == "BookWalker":
                prev_titles = set(it.get("card_titles", []))
    except Exception:
        pass

    candidates = []
    for a in soup.select("a"):
        t = a.get_text(" ", strip=True)
        t = re.sub(r"\s+", " ", (t or "")).strip()
        if not t:
            continue
        if len(t) < 6 or len(t) > 90:
            continue

        # å°è¦½/ç³»çµ±å­—è¸¢æ‰
        if any(bad in t for bad in [
            "æœƒå“¡è³‡æ–™", "æœƒå“¡é€šçŸ¥", "ç™»å…¥", "è¨»å†Š", "æ¨è–¦ä¸»é¡Œ", "æ´»å‹•åˆ—è¡¨", "æŸ¥çœ‹æ›´å¤š", "ä¸‹è¼‰APP"
        ]):
            continue

        candidates.append(t)

    def score(t: str) -> int:
        s = 0
        # æŠ˜æ‰£/åƒ¹æ ¼/é–€æª»/æ—¥æœŸï¼šå¼·è¨Šè™Ÿ
        if re.search(r"\d+\s*æŠ˜", t): s += 6
        if re.search(r"\d+\s*(%|ï¼…)", t): s += 5
        if re.search(r"æ»¿\s*\d+", t): s += 5
        if re.search(r"ç‰¹åƒ¹\s*\d+|å„ªæƒ åƒ¹\s*\d+|\d+\s*å…ƒ", t): s += 5  # 99å…ƒã€2000å…ƒ
        if re.search(r"\d{1,2}[./]\d{1,2}", t): s += 4
        if re.search(r"\d{4}[./]\d{1,2}[./]\d{1,2}", t): s += 4
        if any(k in t for k in ["é™æ™‚", "å„ªæƒ ", "æŠ˜åƒ¹åˆ¸", "å›é¥‹", "æ›¸å±•", "å†æŠ˜", "åŠ ç¢¼", "ç‰¹åƒ¹"]): s += 3

        # æ´»å‹•å‹ï¼ˆä½ å‰›å‰›é»åçš„é–±è®€å ±å‘Š/é»æ•¸åˆ¸ï¼‰ï¼šåŠ åˆ†è®“å®ƒä¸æœƒè¢«æ“ æ‰
        if any(k in t for k in ["é–±è®€å ±å‘Š", "é»æ•¸", "é ˜åˆ¸", "å„ªæƒ åˆ¸", "æŠ½ç", "ä»»å‹™"]): s += 6

        # ä¸è¦å†ç”¨ã€Œæ—¥æ–‡æ›¸ã€æ‰£åˆ†ï¼ˆå®ƒæœ‰æ™‚å°±æ˜¯æ­£å¸¸æ´»å‹•æ¨™é¡Œçš„ä¸€éƒ¨åˆ†ï¼‰
        # ä½†ä»ç„¶ä¿ç•™å°ã€Œé™åˆ¶ç´š/é€£è¼‰ã€çš„è² åˆ†ä»¥é¿å…æ´—ç‰ˆ
        if any(k in t for k in ["é™åˆ¶ç´š", "é€£è¼‰"]): s -= 6

        return s

    scored = [(score(t), t) for t in candidates]
    scored = [(sc, t) for (sc, t) in scored if sc > 0]
    scored.sort(key=lambda x: x[0], reverse=True)
    # æ–°æ´»å‹•ï¼šä»Šå¤©æœ‰ã€æ˜¨å¤©æ²’æœ‰
    new_items = [t for (_, t) in scored if t not in prev_titles]

    # ä¿è­‰æœ€å¤šä¿ç•™ 2 å€‹æ–°æ´»å‹•
    guaranteed_new = new_items[:2]

    # åˆ†å…©é¡ï¼šä¿ƒéŠ·å‹ vs æ´»å‹•å‹
    promo_like = []
    activity_like = []
    for sc, t in scored:
        if any(k in t for k in ["é–±è®€å ±å‘Š", "é»æ•¸", "é ˜åˆ¸", "å„ªæƒ åˆ¸", "æŠ½ç", "ä»»å‹™"]):
            activity_like.append(t)
        else:
            promo_like.append(t)

    # ä¿ƒéŠ·å‹å…ˆå– 6ï¼Œæ´»å‹•å‹è£œ 2ï¼ˆé¿å…æ¼æ‰ä½ åœ¨æ„çš„æ´»å‹•ï¼‰
    picked = guaranteed_new + promo_like

    return pick_unique_texts(picked, limit=15)


def extract_readmoo_cards(html: str) -> List[str]:
    import re
    import json

    # Readmoo å¸¸è¦‹å…©ç¨®ç‹€æ…‹ï¼š
    # A) æœ‰ READMOO_CAMPAIGNSï¼ˆå¯æŠ½ï¼‰
    # B) è¢«æ“‹ï¼ˆåªæœ‰é©—è­‰/JS æç¤ºé ï¼‰ -> æŠ½ä¸åˆ°
    h_lower = (html or "").lower()
    if "verify that you're not a robot" in h_lower or "enable javascript" in h_lower:
        return []

    # æ”¾å¯¬ï¼šæŠ“åˆ°ç¬¬ä¸€å€‹ ]; ç‚ºæ­¢ï¼Œä¸è¦æ±‚ä¸€å®šæ˜¯ [{...}];
    m = re.search(r"const\s+READMOO_CAMPAIGNS\s*=\s*(\[[\s\S]*?\]);", html)
    if not m:
        return []

    raw = m.group(1)
    try:
        data = json.loads(raw)
    except Exception:
        return []

    cards = []
    for item in data:
        name = (item.get("name") or "").strip()
        desc = (item.get("description") or "").strip()
        start = (item.get("start_date") or "").strip()
        end = (item.get("end_date") or "").strip()

        line = " ".join(x for x in [
            name,
            desc,
            f"{start}â€“{end}" if start or end else ""
        ] if x)

        if line:
            cards.append(line)

    return cards

def extract_hyread_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    for a in soup.select("a"):
        href = (a.get("href") or "")
        txt = a.get_text(" ", strip=True)
        txt = re.sub(r"\s+", " ", (txt or "")).strip()
        if not txt:
            continue
        if len(txt) < 6 or len(txt) > 80:
            continue

        # æ’é™¤å°è¦½/ç³»çµ±
        if any(bad in txt for bad in ["ç™»å…¥", "è¨»å†Š", "æœƒå“¡", "æœå°‹", "å®¢æœ", "æ›´å¤š", "è¿”å›"]):
            continue

        # HyRead æ´»å‹•é å¤šåŠæ˜¯ event / store / Template ç›¸é—œé€£çµï¼›ä¸ç¡¬ç¶ä½†åŠ æ¬Š
        if "event" in href or "Template" in href or "store" in href:
            candidates.append(txt)
        else:
            # ä»ç„¶æ”¶ä¸€äº›çœ‹èµ·ä¾†åƒæ´»å‹•çš„
            if any(k in txt for k in ["æŠ˜", "å„ªæƒ ", "æ´»å‹•", "é™æ™‚", "å›é¥‹", "æ»¿", "ç‰¹åƒ¹"]):
                candidates.append(txt)

    return pick_unique_texts(candidates, limit=12)

def extract_books_cards(html: str) -> List[str]:
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    for a in soup.select("a"):
        txt = a.get_text(" ", strip=True)
        txt = re.sub(r"\s+", " ", (txt or "")).strip()
        if not txt:
            continue
        if len(txt) < 6 or len(txt) > 80:
            continue

        if any(bad in txt for bad in ["ç™»å…¥", "è¨»å†Š", "æœƒå“¡", "è³¼ç‰©è»Š", "å®¢æœ", "æ›´å¤š", "è¿”å›"]):
            continue

        # åªä¿ç•™æ¯”è¼ƒåƒæ´»å‹•çš„
        if any(k in txt for k in ["æŠ˜", "å„ªæƒ ", "æ´»å‹•", "æ›¸å±•", "ç‰¹åƒ¹", "å›é¥‹", "æ»¿", "é™æ™‚"]):
            candidates.append(txt)

    return pick_unique_texts(candidates, limit=12)

def load_prev_signature() -> Dict[str, Any]:
    if not os.path.exists(OUT_JSON):
        return {"parser_version": None, "sig": {}}
    try:
        with open(OUT_JSON, "r", encoding="utf-8") as f:
            prev = json.load(f)
        sig = {}
        for it in prev.get("items", []):
            platform = it.get("platform", "")
            signature = it.get("signature", "")
            if platform:
                sig[platform] = signature
        return {"parser_version": prev.get("parser_version"), "sig": sig}
    except Exception:
        return {"parser_version": None, "sig": {}}


def make_signature(page_title: str, card_titles: List[str], status: int, error: str) -> str:
    # ç”¨ã€Œæœ€èƒ½ä»£è¡¨ä»Šæ—¥ç‹€æ…‹ã€çš„è³‡è¨Šåšç°½å
    base = {
        "status": status,
        "title": page_title or "",
        "cards": card_titles[:8],
        "error": (error or "")[:120],
    }
    return json.dumps(base, ensure_ascii=False, sort_keys=True)


def main():
    tz = timezone(timedelta(hours=8))  # å°ç£æ™‚é–“
    now = datetime.now(tz).strftime("%Y-%m-%d %H:%M")

    prev = load_prev_signature()
    prev_sig = prev["sig"]
    prev_ver = prev["parser_version"]

    items = []
    changed_platforms = []

    for x in URLS:
        title = ""
        error = ""
        status = 0
        card_titles: List[str] = []

        try:
            res = fetch_html(x["url"])
            html = res["text"]
            status = res["status"]
            title = extract_title(html)

            if x.get("extra") == "bw":
                card_titles = extract_bw_cards(html)
            elif x.get("extra") == "readmoo":
                card_titles = extract_readmoo_cards(html)

        except requests.HTTPError as e:
            # ä¾‹å¦‚ 403
            error = str(e)
            try:
                status = e.response.status_code if e.response is not None else 0
            except Exception:
                status = 0
        except Exception as e:
            error = str(e)

        signature = make_signature(title, card_titles, status, error)

        if (
            prev_ver == PARSER_VERSION
            and prev_sig.get(x["platform"])
            and prev_sig.get(x["platform"]) != signature
        ):
            changed_platforms.append(x["platform"])

        if x["platform"] == "Readmoo":
            # å¦‚æœæŠ½ä¸åˆ°æ´»å‹•ï¼Œé †ä¾¿åˆ¤æ–·æ˜¯ä¸æ˜¯è¢«æ“‹
            if ("READMOO_CAMPAIGNS" not in html) and (
                "verify that you're not a robot" in html.lower()
                or "enable javascript" in html.lower()
            ):
                error = "Readmoo ç–‘ä¼¼åæ©Ÿå™¨äºº/JS é©—è­‰ï¼ŒActions æŠ“åˆ°çš„ä¸æ˜¯æ´»å‹•é å…§å®¹"

        blocked = False
        blocked_reason = ""

        if x["platform"] == "Readmoo":
            # åªè¦æŠ“åˆ°çš„ä¸æ˜¯æ´»å‹•é æœ¬é«”ï¼Œå°±è¦–ç‚º blocked
            if error and ("robot" in error.lower() or "javascript" in error.lower() or "js" in error.lower()):
            blocked = True
            blocked_reason = "éœ€è¦ JavaScript é©—è­‰ï¼ŒActions ç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"
        else:
            # æ²’æœ‰ error ä¹Ÿå¯èƒ½æ‹¿åˆ°é©—è­‰é ï¼ˆ200 OKï¼‰
            h = (html or "").lower() if "html" in locals() else ""
            if "verify that you're not a robot" in h or "enable javascript" in h:
                blocked = True
                blocked_reason = "éœ€è¦ JavaScript é©—è­‰ï¼ŒActions ç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"
            # æˆ–æ˜¯æ ¹æœ¬æ²’æœ‰ READMOO_CAMPAIGNSï¼ˆä½ èµ° JS è®Šæ•¸æŠ½å–é‚£æ¢è·¯æ™‚å¾ˆæœ‰ç”¨ï¼‰
            if (not blocked) and ("readmoo_campaigns" not in h):
            # é€™æ¢æ¯”è¼ƒä¿å®ˆï¼šåªæœ‰ç•¶ card_titles ä¹Ÿç©ºæ‰åˆ¤å®š
                if not card_titles:
                    blocked = True
                    blocked_reason = "ç–‘ä¼¼åæ©Ÿå™¨äºº/JS é©—è­‰ï¼Œç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"

        elif x.get("extra") == "hyread":
            card_titles = extract_hyread_cards(html)

        elif x.get("extra") == "books":
            card_titles = extract_books_cards(html)

        items.append(
            {
                "platform": x["platform"],
                "url": x["url"],
                "note": x["note"],
                "page_title": title,
                "card_titles": card_titles,
                "http_status": status,
                "error": error,
                "signature": signature,
                "blocked": blocked,
                "blocked_reason": blocked_reason,
            }
        )

    os.makedirs("data", exist_ok=True)

    payload = {
        "parser_version": PARSER_VERSION,
        "updated_at_taipei": now,
        "has_new_changes": "æ˜¯" if len(changed_platforms) > 0 else "å¦",
        "changed_platforms": changed_platforms,
        "items": items,
    }

    with open(OUT_JSON, "w", encoding="utf-8") as f:
        json.dump(payload, f, ensure_ascii=False, indent=2)

    # HTML ä¸€é æ¸…å–®ï¼ˆå«å¼±åŒ– 403ï¼‰
    html_lines = []
    html_lines.append("<!doctype html>")
    html_lines.append('<html lang="zh-Hant">')
    html_lines.append("<head>")
    html_lines.append('<meta charset="utf-8" />')
    html_lines.append('<meta name="viewport" content="width=device-width, initial-scale=1" />')
    html_lines.append("<title>é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</title>")
    html_lines.append("</head>")
    html_lines.append("<body style='font-family:-apple-system,BlinkMacSystemFont,Segoe UI,Roboto,Noto Sans TC,Helvetica,Arial;line-height:1.65;padding:16px;max-width:980px;margin:0 auto;'>")
    html_lines.append("<h1 style='margin:0 0 8px;'>ğŸ“š é›»å­æ›¸å¹³å°æ´»å‹•å¿«ç…§</h1>")
    html_lines.append(f"<p style='margin:0 0 8px;'>æ›´æ–°æ™‚é–“ï¼ˆå°ç£ï¼‰ï¼š<b>{payload['updated_at_taipei']}</b></p>")
    html_lines.append(f"<p style='margin:0 0 16px;'>ä»Šå¤©æ˜¯å¦æœ‰æ–°å¢æ´»å‹•ï¼š<b>{payload['has_new_changes']}</b>"
                      + (f"ï¼ˆè®Šå‹•ï¼š{', '.join(payload['changed_platforms'])}ï¼‰" if payload["changed_platforms"] else "")
                      + "</p>")
    html_lines.append("<hr style='opacity:.35'/>")

    for it in items:
        is_blocked = bool(it.get("blocked"))
        is_403 = (it.get("http_status") == 403) or ("403" in (it.get("error") or ""))
        # å¼±åŒ–ï¼š403 è®Šç° + é™ä½é€æ˜åº¦
        wrap_style = "opacity:.45; filter: grayscale(1);" if is_403 else "opacity:1;"
        title_style = "color:#555;" if is_403 else "color:#111;"

        html_lines.append(f"<section style='{wrap_style} padding:8px 0;'>")
        html_lines.append(f"<h2 style='margin:6px 0; {title_style}'>{it['platform']}</h2>")

        if it["page_title"]:
            html_lines.append(f"<p style='margin:4px 0;'>é é¢æ¨™é¡Œï¼š{it['page_title']}</p>")
        if it["note"]:
            html_lines.append(f"<p style='margin:4px 0;'>å‚™è¨»ï¼š{it['note']}</p>")

        # é¡å¤–æŠ“å¡ç‰‡æ¨™é¡Œï¼ˆåªåœ¨ BW/Readmoo æœ‰ï¼‰
        # æ¨¡å¼ 3ï¼šReadmoo è‹¥ blockedï¼Œå°±ä¸é¡¯ç¤ºå¡ç‰‡å€å¡Šï¼Œåªé¡¯ç¤ºåŸå› ï¼‹é€£çµ
        if it["platform"] == "Readmoo" and is_blocked:
            reason = it.get("blocked_reason") or "éœ€è¦ JavaScript é©—è­‰ï¼Œç„¡æ³•å–å¾—æ´»å‹•æ¸…å–®"
            html_lines.append(f"<p style='margin:6px 0; color:#666;'>ï¼ˆ{reason}ï¼‰</p>")
        else:
            if it.get("card_titles"):
                html_lines.append("<div style='margin:8px 0 6px;'><b>æ´»å‹•å¡ç‰‡ï¼ˆæ“·å–ï¼‰</b></div>")
                html_lines.append("<ul style='margin:6px 0 10px 18px;'>")
                for t in it["card_titles"][:10]:
                    html_lines.append(f"<li>{t}</li>")
                html_lines.append("</ul>")

        html_lines.append(f"<p style='margin:6px 0;'><a href='{it['url']}' target='_blank' rel='noopener noreferrer'>â†’ é»æˆ‘æŸ¥çœ‹æ´»å‹•</a></p>")

        if it["error"] and not (it["platform"] == "Readmoo" and is_blocked):
            html_lines.append(f"<p style='margin:6px 0; color:#b00020;'>ï¼ˆæŠ“å–å¤±æ•—ï¼š{it['error']}ï¼‰</p>")

        html_lines.append("</section>")
        html_lines.append("<hr style='opacity:.25'/>")

    html_lines.append("<p style='font-size:12px;opacity:.7;margin-top:14px;'>v1 åªå½™æ•´å®˜æ–¹æ´»å‹•å…¥å£ï¼›ä¸è¨ˆåˆ¸å¾Œåƒ¹èˆ‡å–®æ›¸ç‰¹åƒ¹ã€‚403 å¹³å°å·²è‡ªå‹•å¼±åŒ–é¡¯ç¤ºã€‚</p>")
    html_lines.append("</body></html>")

    with open(OUT_HTML, "w", encoding="utf-8") as f:
        f.write("\n".join(html_lines))


if __name__ == "__main__":
    main()
